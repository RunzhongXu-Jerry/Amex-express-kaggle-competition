# -*- coding: utf-8 -*-
"""xgb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BdRt7Zu_qANa5dSu5rtqwirfpHuYRK_w
"""

import os,random 
import gc
import warnings
import tqdm 
import pandas as pd
import xgboost as xgb
import numpy as np
import joblib
import pathlib
import tqdm

import time 
import torch, gc 
torch.cuda.empty_cache()
gc.collect()  

from sklearn.model_selection import StratifiedKFold,train_test_split
from itertools import combinations
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
import warnings; warnings.filterwarnings('ignore')

class CFG:
  seed = 42
  TRAIN = True 
  INFER = True
  n_folds = 5
  target ='target'
  DEBUG= False 
  ADD_CAT = True
  ADD_LAG = True 
  ADD_DIFF =  [1, 2]
  ADD_MIDDLE = True
  INPUT = "../data"
  model_dir = "../result/model"
  sub_dir = "../result/sub"


path = f'{CFG.INPUT}'

def get_not_used():
  return ['customer_ID','target','S_2','D_103','D_139']

if CFG.TRAIN:
  train = pd.read_pickle("./train_fe.pickle")
  print(train.shape)
  train.head()

if CFG.DEBUG:
  train = train.sample(n=2000,random_state=42).reset_index(drop=True)
  features = [col for col in train.columns if col not in get_not_used()]

if CFG.INFER:
  test = pd.read_pickle("./test_fe.pickle")
  print(test.shape)
  test.head()

# ====================================================
# Seed everything
# ====================================================
def seed_everything(seed):
  random.seed(seed)
  np.random.seed(seed)
  os.environ['PYTHONHASHSEED'] = str(seed)

seed_everything(CFG.seed)

def fillna_and_reduce_memory(df):
  df.fillna(0,inplace=True)
  #replace INF with zeros
  df.replace([np.inf,-np.inf],0,inplace=True)
  #reduce memory
  for col in df.columns:
    if col in get_not_used(): continue
    if str(df[col].dtype) == 'int64':
      df[col] = df[col].astype('int32')
    if str(df[col].dtype) == 'float64':
      df[col] = df[col].astype('float32')
  return df

train = fillna_and_reducememory(train)
test = fillna_and_reducememory(test)

params = {
    #'booster': 'dart',
    'objective': 'binary:logistic', 
    'tree_method': 'gpu_hist', 
    'max_depth': 9,
    'subsample':0.6,
    'colsample_bytree': 0.6,
    'gamma':1.5,
    'min_child_weight':8,
    'lambda':70,
    'eta':0.02, 
}

def xgb_train(xt,yt,xv,yv,_params=params):
  assert xt.shape[1] == xv.shape[1]
  dtrain = xgb.DMatrix(data=xt,label=yt)
  dvalid = xgb.DMatrix(data=xv,label=yv)

  watchlist = [(dtrain, 'train'), (dvalid, 'eval')]
  bst = xgb.train(_params, dtrain=dtrain,
          num_boost_round=4000,evals=watchlist,
          early_stopping_rounds=600, feval=xgb_amex, maximize=True,
          verbose_eval=100)
  valid_pred = bst.predict(dvalid,iteration_range=(0,bst.best_ntree_limit))
  return valid_pred,bst

# https://www.kaggle.com/kyakovlev
# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534
def amex_metric(y_true, y_pred):
  labels = np.transpose(np.array([y_true, y_pred]))
  labels = labels[labels[:, 1].argsort()[::-1]]
  weights = np.where(labels[:,0]==0, 20, 1)
  cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]
  top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])
  gini = [0,0]
  for i in [1,0]:
      labels = np.transpose(np.array([y_true, y_pred]))
      labels = labels[labels[:, i].argsort()[::-1]]
      weight = np.where(labels[:,0]==0, 20, 1)
      weight_random = np.cumsum(weight / np.sum(weight))
      total_pos = np.sum(labels[:, 0] *  weight)
      cum_pos_found = np.cumsum(labels[:, 0] * weight)
      lorentz = cum_pos_found / total_pos
      gini[i] = np.sum((lorentz - weight_random) * weight)
  return 0.5 * (gini[1]/gini[0] + top_four)

def amex_metric_np(preds, target):
  indices = np.argsort(preds)[::-1]
  preds, target = preds[indices], target[indices]
  weight = 20.0 - target * 19.0
  cum_norm_weight = (weight / weight.sum()).cumsum()
  four_pct_mask = cum_norm_weight <= 0.04
  d = np.sum(target[four_pct_mask]) / np.sum(target)
  weighted_target = target * weight
  lorentz = (weighted_target / weighted_target.sum()).cumsum()
  gini = ((lorentz - cum_norm_weight) * weight).sum()
  n_pos = np.sum(target)
  n_neg = target.shape[0] - n_pos
  gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)
  g = gini / gini_max
  return 0.5 * (g + d)

def xgb_amex(y_pred,y_true):
  return amex_metric(y_true.get_label(),y_pred)

def train_fn(i,x,y,x_true,y_true,_params=params):
  valid_pred,bst = xgb_train(x,y,x_true,y_true,_params)
  bst.save_model(f'XGB_v{CFG.ver}_fold{fold}.json')
  amex_score = amex_metric(y_true.get_label(),valid_pred)
  return amex_score, bst, valid_pred

if CFG.TRAIN:
  # a numpy array to store out of folds predictions
  oof_predictions = np.zeros(len(train))
  # a numpy array to store test predictions
  test_predictions = np.zeros(len(test))
  features = [col for col in train.columns if col not in get_not_used()]
  kfold = StratifiedKFold(n_splits=CFG.n_folds,shuffle=True,random_state=CFG.seed)
  for fold,(trn_ind,val_ind) in enumerate(kfold.split(train,train[CFG.target])):
    x_train,y_train = train[features].iloc[trn_ind],train[CFG.target].iloc[trn_ind]
    x_val,y_val = train[features].iloc[val_ind],train[CFG.target].iloc[val_ind]
    amex_score,model,pvalid = train_fn(fold,xtrain,ytrain,xvalid,yvalid)
    oof_predictions[val_ind] = pvalid
    score += amex_score
    del x_train,y_train,x_val,y_val
    gc.collect()
    torch.cuda.empty_cache()
  score /= CFG.n_folds
  oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})
  oof_df.to_csv(f'xg_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)   
  print(f"Average amex score: {score:.4f}")

if CFG.INFER:
  test_predictions = np.zeros(len(test))
  yps = 
  not_used = get_not_used()
  not_used = [i for i in not_used if i in test.columns]
  for fold in range(CFG.n_folds):
    model = xgb.Booster()
    model.load_model(f'XGB_v{CFG.ver}_fold{fold}.json')
    dtest = xgb.DMatrix(test.drop(not_used,axis=1))
    test_pred += bst.predict(dtest,iteration_range=(0,bst.best_ntree_limit))
  test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})
  test_df.to_csv(f'test_xg_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)







